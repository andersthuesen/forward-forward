{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = MNIST(root=\"./data\", download=True, transform=T.Compose([T.ToTensor(), T.Normalize(0.1307, 0.3014)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = DataLoader(mnist_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list(list: List, value = 0, size: int = 1):\n",
    "  return [value] * size + list + [value] * size\n",
    "\n",
    "def window_list(list: List, size: int):\n",
    "  return zip(*[list[i:] for i in range(size)])\n",
    "\n",
    "\n",
    "class FFLinear(nn.Module):\n",
    "  def __init__(self, in_features, out_features, threshold, *args, **kwargs):\n",
    "    self.linear = nn.Linear(in_features, out_features)\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "    self.threshold = threshold\n",
    "\n",
    "  def prob_good(self, x):\n",
    "    return F.sigmoid(self.goodness(x) - self.threshold)\n",
    "\n",
    "  def goodness(self, x):\n",
    "    n = F.normalize(x, 2)\n",
    "    h = self.linear(n)\n",
    "    a = self.relu(h)\n",
    "    return (a**2).sum()\n",
    "\n",
    "  def forward(self, x):\n",
    "    n = F.normalize(x, 2)\n",
    "    h = self.linear(n)\n",
    "    a = self.relu(h)\n",
    "    return a\n",
    "\n",
    "class RecurrentFFModel(nn.Module):\n",
    "  def __init__(self, input_size: int, layer_sizes: List[layer_size], rollouts=8,alpha=0.8, lr=1e-2):\n",
    "    super(RecurrentFFModel, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.rollouts = rollouts\n",
    "    self.alpha = alpha\n",
    "    self.lr = lr\n",
    "    # TODO: Make this more readable.\n",
    "    self.layers: List[FFLinear] = [\n",
    "      FFLinear(up_input_size + down_input_size, output_size)\n",
    "      for (up_input_size, output_size, down_input_size) in window_list([self.input_size] + layer_sizes + [0], 3)\n",
    "    ]\n",
    "\n",
    "  def forward(self, X: torch.Tensor):\n",
    "    # Initialize layer activities\n",
    "    activity = [X] + [torch.zeros(X.shape[0], l.out_features) for l in self.layers] + [torch.zeros(X.shape[0], 0)]\n",
    "\n",
    "    # Rollout time\n",
    "    for rollout in range(self.rollouts):\n",
    "      new_activity = activity.copy()\n",
    "      for i, layer in enumerate(self.layers): # This could be done in parallel\n",
    "        # TODO: This should probably be moved to the linear layer class\n",
    "        j = i + 1 # Activity index\n",
    "\n",
    "        # For illustration purposes we don't rely on torch's autofiff, though it might be faster.\n",
    "        with torch.no_grad(): \n",
    "          layer_input = torch.cat((activity[j-1], activity[j+1]), dim=1)\n",
    "          norm_layer_input = layer_input / (1e-8 + torch.linalg.norm(layer_input, 2)) # Make sure vector has length 1.\n",
    "          z = layer(norm_layer_input)\n",
    "          y = F.relu(z)\n",
    "          #loss = y**2\n",
    "\n",
    "          # Compute gradient manually\n",
    "          #                   dl/dW             dl/dy   dy/dz           dz/dW\n",
    "          dldW = torch.einsum(\"na,na,nb->nab\",  y,      ((z > 0) * 1),  norm_layer_input).mean(dim=0)\n",
    "          #dl/db dl/dy dy/dz          dz/db\n",
    "          dldb = (y * ((z > 0) * 1) * 1).mean(dim=0) #torch.einsum(\"na,na->na\",  y,      ((z > 0) * 1)).mean(dim=0)\n",
    "          layer.weight -= self.lr * dldW # Update the weights\n",
    "          layer.bias -= self.lr * dldb # Update the weights\n",
    "\n",
    "        new_activity[j] = y\n",
    "\n",
    "      activity = new_activity # Switch activities\n",
    "\n",
    "    return activity[1:-1] # Only return hidden activities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentFFModel(input_size=28*28, layer_sizes=[400, 200, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-4.1034e-05, grad_fn=<MeanBackward0>) tensor(0.0184, grad_fn=<StdBackward0>)\n",
      "tensor(-0.0008, grad_fn=<MeanBackward0>) tensor(0.0186, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight.mean(), model.layers[0].weight.std())\n",
    "print(model.layers[0].bias.mean(), model.layers[0].bias.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb3763298e24337910bba7503bb1055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image, label in tqdm(mnist_data):\n",
    "  image = image.flatten(start_dim=1) # Flatten image\n",
    "  model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3312e-05, grad_fn=<MeanBackward0>) tensor(0.0184, grad_fn=<StdBackward0>)\n",
      "tensor(-0.0182, grad_fn=<MeanBackward0>) tensor(0.0060, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight.mean(), model.layers[0].weight.std())\n",
    "print(model.layers[0].bias.mean(), model.layers[0].bias.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of dividing the above into multiple layers, we could have one big matrix of connection weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
