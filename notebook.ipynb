{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "from typing import List, Union\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = MNIST(root=\"./data\", download=True, transform=T.Compose([T.ToTensor(), T.Normalize(0.1307, 0.3014)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = DataLoader(mnist_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFLayer(nn.Module):\n",
    "  class GoodnessMeasure(Enum):\n",
    "    SUM_OF_SQUARED_ACTIVITIES = 1\n",
    "    SUM_OF_ACTIVITIES = 2\n",
    "\n",
    "  class MaxObjective(Enum):\n",
    "    LIKELIHOOD = 1\n",
    "    LOG_LIKELIHOOD = 2\n",
    "    GOODNESS = 3\n",
    "    NEGATIVE_GOODNESS = 4\n",
    "\n",
    "  def __init__(self, in_features, out_features, threshold=2, \n",
    "              good_measure=GoodnessMeasure.SUM_OF_SQUARED_ACTIVITIES, max_obj=MaxObjective.LOG_LIKELIHOOD, lr=1e-3):\n",
    "\n",
    "    super(FFLayer, self).__init__()\n",
    "    self.in_features = in_features\n",
    "    self.out_features = out_features\n",
    "    self.linear = nn.Linear(in_features, out_features)\n",
    "    self.activation = nn.ReLU(inplace=True)\n",
    "    self.opt = SGD(self.linear.parameters(), lr=lr)\n",
    "\n",
    "    self.good_measure = good_measure\n",
    "    self.threshold = threshold\n",
    "    self.max_obj = max_obj\n",
    "\n",
    "  def prob_positive(self, x):\n",
    "    goodness, activity = self.goodness(x)\n",
    "    return torch.sigmoid(goodness - self.threshold), activity\n",
    "\n",
    "  def goodness(self, x):\n",
    "    activity = self.forward(x)\n",
    "    if self.good_measure == self.GoodnessMeasure.SUM_OF_SQUARED_ACTIVITIES:\n",
    "      goodness = (activity**2).sum()\n",
    "    elif self.good_measure == self.GoodnessMeasure.SUM_OF_ACTIVITIES:\n",
    "      goodness = activity.sum()\n",
    "    return goodness, activity\n",
    "\n",
    "  def train(self, x, negative=False):\n",
    "    sign = -1 if negative else 1\n",
    "    loss = None\n",
    "    activity = None\n",
    "\n",
    "    if self.max_obj == self.MaxObjective.LIKELIHOOD:\n",
    "      prob_pos, activity = self.prob_positive(x)\n",
    "      loss = sign * -prob_pos\n",
    "  \n",
    "    elif self.max_obj == self.MaxObjective.LOG_LIKELIHOOD:\n",
    "      prob_pos, activity = self.prob_positive(x)\n",
    "      loss = sign * -torch.log(prob_pos)\n",
    "\n",
    "    elif self.max_obj == self.MaxObjective.GOODNESS:\n",
    "      goodness, activity = self.goodness(x)\n",
    "      loss = sign * -goodness\n",
    "\n",
    "    elif self.max_obj == self.MaxObjective.NEGATIVE_GOODNESS:\n",
    "      goodness, activity = self.goodness(x)\n",
    "      loss = sign * goodness\n",
    "\n",
    "\n",
    "    self.opt.zero_grad()\n",
    "    loss.backward()\n",
    "    self.opt.step()\n",
    "    return activity\n",
    "\n",
    "  def train_pos(self, x):\n",
    "    return self.train(x, negative=False)\n",
    "\n",
    "  def train_neg(self, x):\n",
    "    return self.train(x, negative=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    n = F.normalize(x, 2) \n",
    "    h = self.linear(n)\n",
    "    a = self.activation(h)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentFFModel(nn.Module):\n",
    "  def __init__(self, input_size: int, layer_sizes: List[layer_size], \n",
    "                rollouts=8, lr=1e-3, max_obj=FFLayer.MaxObjective.LOG_LIKELIHOOD):\n",
    "                \n",
    "    super(RecurrentFFModel, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.rollouts = rollouts\n",
    "    self.lr = lr\n",
    "\n",
    "    self.max_obj = max_obj\n",
    "\n",
    "    \"\"\"\n",
    "    Each layer depends on layers above and below, except top layer.\n",
    "                   ___     ___           <--- Top layer\n",
    "               ___/   \\___/   \\___       <--- Middle layer (could be many)\n",
    "           ___/   \\___/   \\___/   \\___   <--- Bottom layer\n",
    "          /       /       /       /\n",
    "     frame   frame   frame   frame\n",
    "\n",
    "    \"\"\"\n",
    "    self.layers = []\n",
    "    up_size = input_size\n",
    "    for out_size, down_size in zip(layer_sizes, layer_sizes[1:]):\n",
    "      layer = self.make_layer(up_size, down_size, out_size)\n",
    "      self.layers.append(layer)\n",
    "      up_size = out_size\n",
    "    # Top layer only gets input from layer below, hence down_size=0.\n",
    "    top_layer = self.make_layer(up_size=layer_sizes[-2], down_size=0, out_size=layer_sizes[-1])\n",
    "    self.layers.append(top_layer)\n",
    "\n",
    "  def make_layer(self, up_size, down_size, out_size):\n",
    "    return FFLayer(up_size + down_size, out_size, lr=self.lr, max_obj=self.max_obj)\n",
    "\n",
    "  def test(self, x, labels, num_classes):\n",
    "    activities = self.forward(x)\n",
    "    top_activities = activities[-1] # Latent representation of the letter\n",
    "    # Create some kind of K-nearest neighbour clustering where K=num_classes.\n",
    "\n",
    "  def train(self, x):\n",
    "    top_activity = [torch.zeros(x.shape[0], 0)] # TODO: Could take labels for supervised learning\n",
    "    init_layers_activity = [torch.zeros(x.shape[0], l.out_features) for l in self.layers]\n",
    "    \n",
    "    pos_activity = [x] + init_layers_activity + top_activity\n",
    "    neg_activity = [torch.randn_like(x)] + init_layers_activity + top_activity\n",
    "\n",
    "    for rollout in range(self.rollouts):\n",
    "      pos_activity[1:-1] = [\n",
    "        layer.train_pos(torch.cat((pos_activity[i], pos_activity[i+2]), dim=1).detach())\n",
    "        for i, layer in enumerate(self.layers) # TODO: Make this parallel\n",
    "      ]\n",
    "\n",
    "    for rollout in range(self.rollouts):\n",
    "      #neq_activity[0] = torch.randn_like(x) # Give new random input for each timestep\n",
    "      neg_activity[1:-1] = [\n",
    "        layer.train_neg(torch.cat((neg_activity[i], neg_activity[i+2]), dim=1).detach())\n",
    "        for i, layer in enumerate(self.layers)\n",
    "      ]\n",
    "\n",
    "    return pos_activity[1:-1], neg_activity[1:-1]\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Initialize layer activities\n",
    "    top_activity = [torch.zeros(x.shape[0], 0)]\n",
    "    init_layers_activity = [torch.zeros(x.shape[0], l.out_features) for l in self.layers]\n",
    "    activity = [x] + init_layers_activity + top_activity\n",
    "\n",
    "    for rollout in range(self.rollouts):\n",
    "      activity[1:-1] = [\n",
    "        layer(torch.cat((activity[i], activity[i+2]), dim=1).detach()) \n",
    "        for i, layer in enumerate(self.layers)\n",
    "      ]\n",
    "\n",
    "    return activity[1:-1] # Only return hidden activities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentFFModel(input_size=28*28, layer_sizes=[400, 200, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c8cf533b4744bb9dc12c270b39a94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image, label in tqdm(mnist_data):\n",
    "  image = image.flatten(start_dim=1) # Flatten image\n",
    "  model.train(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(mnist_data))\n",
    "image = image.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0413) tensor(0.9752)\n",
      "tensor(-0.0172) tensor(0.9937)\n",
      "[tensor(0.0017, grad_fn=<MeanBackward0>), tensor(0.0032, grad_fn=<MeanBackward0>), tensor(0.0054, grad_fn=<MeanBackward0>)]\n",
      "[tensor(2.2952e-07, grad_fn=<MeanBackward0>), tensor(0., grad_fn=<MeanBackward0>), tensor(2.7183e-07, grad_fn=<MeanBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "rand_image = torch.randn_like(image)\n",
    "print(image.mean(), image.std())\n",
    "print(rand_image.mean(), rand_image.std())\n",
    "\n",
    "pos_activities = model(image)\n",
    "neg_activities = model(rand_image)\n",
    "\n",
    "print([(pos_act ** 2).mean() for pos_act in pos_activities])\n",
    "print([(neg_act ** 2).mean() for neg_act in neg_activities])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
